{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to use SpaCy\n",
    "## An attempt at a tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we need to install these modules or packages to our system, just like we did for wordcloud. But remember, you only need to install/run these lines of codes once. Ideally, they should work on both Collab and jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second line above installed a module for english language, and the fourth line was used to import it. Since Spacy is a language/grammar analysis tool, it is language-specific. Luckily, there is support for several other languages like Spanish, German, Polish, Italian and others. You can look at the languages supported here: https://spacy.io/usage, and you'll also find the right command to install other language modules on the same page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') #nlp stands for natural language processing, but you can name it whatever you want. Just remember what name you give it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, in the line of code above, if you use a different language module, you will have to use a different command. You can find the lines for individual languages on this link: https://spacy.io/usage/models\n",
    "But just as an example, the line of processing text in Italian would look like this: nlp = spacy.load(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a sample sentence. It is intriguing and exciting to use Spacy as a tool.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentence = nlp(sentence)\n",
    "for word in processed_sentence:\n",
    "    print(word.text, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the block above, we've used the nlp as a function which processes the sentence. Then, we have used a simple \"for loop\" to extract each element of the sentence and run the same commands on it. Next, we have used two nlp attributes (.text and .pos_), the first which gives us the word being analysed and the second tells us its part of speech (Hence pos). \n",
    "\n",
    "But the example above is very basic and does not seem super-helpful. So what if we know the part of speech? But if we use just these two attributes to extract all adjectives used by a leader of state in his speech, we could perhaps discover some interesting patterns. In the next section, we take one of the speeches from the dataframe used in Notebook 6 (from week 4) and get a list of all adjectives used. I'm assuming everyone has downloaded the .csv file required for that class from the [Github link](https://github.com/uvacw/datajournalism/blob/master/06-analyzing_text.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Speeches_UK_Cleaned.csv', header = None)\n",
    "df.columns = ['what','when','country','who','number', 'text', 'text_clean','language']\n",
    "speech = df.iloc [1]['text_clean'] #Takes the value of the cell from row 1 and the 'text-clean' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_speech = nlp(speech)\n",
    "speech_adj = [] #Creating an empty list-type variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in processed_speech:\n",
    "    if word.pos_ == \"ADJ\":\n",
    "        speech_adj.append(word.text)\n",
    "\n",
    "print(speech_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block above, we're first extracting single words using the for loop, and then for each word, we're checking if its part-of-speech value is adjective or not. If it is an adjective, then we're adding it to the list *speech_adj*. If it is not an adjective, then it just moves to the next word. The list of what different values are called is available here: https://spacy.io/api/annotation; but just for quick reference some of the most common ones are:\n",
    "\n",
    "\"PROPN\" = Proper noun; \n",
    "<br>\"NOUN\" = Noun;\n",
    "<br>\"ADV\" = Adverb; \n",
    "<br>\"ADJ\" = Adjective; \n",
    "<br>\"VERB\" = Verb; \n",
    "<br>\"NUM\" = Cardinal number; \n",
    "<br>\"PUNCT\" = Punctuation; \n",
    "<br>\"PRON\" = Pronoun; \n",
    "\n",
    "\n",
    "Using this list, you can check for various parts of speech like nouns (to know what countries, people, places some text/string is talking about), verbs (what actions are being spoken of), and so on.\n",
    "\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "Another handy tool from Spacy that allows us to separate \"named entities\" from text. Named entities are essentially \"objects\" that have an assigned name. It's like a proper noun, but more. Therefore, it can be used to identify countries, organizations, and even single out references to money. In the example below, named entities from the speech are identified along with their type, but as you'll see in the results, it is not always perfect in its function. The problem is that these tools were trained on a certain sample of texts, and therefore, may not necessarily be representative of all data we analyse. So, it is always advisable to clean the results derived from this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in processed_speech.ents:\n",
    "    print (named_entity.text, named_entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block above does provide labels, but what do they stand for? You can use the following line to get an explaination:\n",
    "\n",
    "spacy.explain(\"\")\n",
    "\n",
    "In the inverted commas, you can insert the value or label that you need an expanded version for. Try it with \"GPE\" (Also works for the part of speech labels)\n",
    "\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra lines of code:\n",
    "###### Labeling  all words in a sentence/speech (Remember that there is a 100,000 character limit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {}\n",
    "for w in processed_sentence:\n",
    "    if w.pos_ in pos_dict:\n",
    "        pos_dict[w.pos_].append(w.text)\n",
    "    else:\n",
    "        pos_dict [w.pos_] = [w.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over here, an empty dictionary (pos_dict) was created. [*Note that each dictionary has a key and a value. What's interesting is that a key's value can be a list rather than just a single value. That's what helps us categorise and store each word's value in a single dictionary variable.*]\n",
    "\n",
    "After that, each word from the processed string was separated. Its part of speech was identified. Then, if that part of speech [Ex: Noun]'key' existed in the dictionary, the word was added to the list whose value was assigned to the key [Ex: tool; from the sentence above]. But if the part of speech was being identified for the first time in the sentence, a \"key\" of that part of speech was created and then the word was assigned as its value.\n",
    "\n",
    "##### How to use spaCy to look for a certain POS value in all speeches from a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = df.shape [0]\n",
    "speech_verb = []\n",
    "for i in range(len_df):\n",
    "    speech = nlp(df.iloc[i]['text_clean'])\n",
    "    for w in speech:\n",
    "        if w.pos_ == \"VERB\":\n",
    "            speech_verb.append(w.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df.shape** gives us the number of rows and columns respectively, and we store its 0th value, that is the number of rows, in **len_df**. Then, we create a list variable (speech_verb) to store all verbs in the dataframe. \n",
    "The **range(x)** function in the 1st for-loop returns a sequence of numbers starting from 0 to the value of x. Thus,we can use it to run the for-loop each individual row in the dataframe. Additionally, the value of \"i\" in the 1st for-loop can also be used to extract individual speeches from the dataframe. In this way, we process each speech one at a time in the dataframe and run the same set of commands on it. The remaining commands are exactly the same as the ones above, only this time, we are looking for verbs instead of adjectives, and then we're storing it in the speech_verb list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
